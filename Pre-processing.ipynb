{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fetching Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>mark</th>\n",
       "      <th>label</th>\n",
       "      <th>offset1</th>\n",
       "      <th>offset2</th>\n",
       "      <th>span</th>\n",
       "      <th>code</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>es-S0212-71992007000100007-1</td>\n",
       "      <td>T1</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "      <td>40</td>\n",
       "      <td>61</td>\n",
       "      <td>arterial hypertension</td>\n",
       "      <td>38341003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>es-S0212-71992007000100007-1</td>\n",
       "      <td>T2</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "      <td>66</td>\n",
       "      <td>79</td>\n",
       "      <td>polyarthrosis</td>\n",
       "      <td>36186002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>es-S0212-71992007000100007-1</td>\n",
       "      <td>T3</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "      <td>1682</td>\n",
       "      <td>1698</td>\n",
       "      <td>pleural effusion</td>\n",
       "      <td>60046008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>es-S0212-71992007000100007-1</td>\n",
       "      <td>T4</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "      <td>1859</td>\n",
       "      <td>1875</td>\n",
       "      <td>pleural effusion</td>\n",
       "      <td>60046008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>es-S0212-71992007000100007-1</td>\n",
       "      <td>T5</td>\n",
       "      <td>ENFERMEDAD</td>\n",
       "      <td>1626</td>\n",
       "      <td>1648</td>\n",
       "      <td>lower lobe atelectasis</td>\n",
       "      <td>46621007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       filename mark       label  offset1  offset2  \\\n",
       "0  es-S0212-71992007000100007-1   T1  ENFERMEDAD       40       61   \n",
       "1  es-S0212-71992007000100007-1   T2  ENFERMEDAD       66       79   \n",
       "2  es-S0212-71992007000100007-1   T3  ENFERMEDAD     1682     1698   \n",
       "3  es-S0212-71992007000100007-1   T4  ENFERMEDAD     1859     1875   \n",
       "4  es-S0212-71992007000100007-1   T5  ENFERMEDAD     1626     1648   \n",
       "\n",
       "                     span      code  \n",
       "0   arterial hypertension  38341003  \n",
       "1           polyarthrosis  36186002  \n",
       "2        pleural effusion  60046008  \n",
       "3        pleural effusion  60046008  \n",
       "4  lower lobe atelectasis  46621007  "
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities = pd.read_csv('data/entities.tsv',sep = '\\t')\n",
    "entities.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_path = 'data/text/'\n",
    "text_files = list(Path(text_path).glob('*.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_text = {}\n",
    "\n",
    "for file in text_files:\n",
    "    file = str(file)\n",
    "    with open(file, \"r\", encoding=\"UTF-8\") as f:\n",
    "        file_name = file[len(text_path):-4]\n",
    "        file_text.update({file_name: f.read()})\n",
    "\n",
    "text_files = [str(text_file)[len(text_path):-4] for text_file in text_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Splitting & Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(file_name):\n",
    "    text = file_text[file_name]\n",
    "    doc = nlp(text)\n",
    "    sentence_ids = []\n",
    "    tokens = []\n",
    "    for i, sent in enumerate(doc.sents):\n",
    "        sentence_ids.append(i)\n",
    "        tokens.append([(tk.text, tk.idx) for tk in sent])\n",
    "    return sentence_ids, tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Spans to IOB Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bio_tags(tokens, entities):\n",
    "    tags = []\n",
    "    curr_entity = None\n",
    "    for token in tokens:\n",
    "        if len(entities):\n",
    "            nxt_entity = entities[0]\n",
    "            start, end, lbl = nxt_entity[0], nxt_entity[1], nxt_entity[2]\n",
    "            if token[1] >= start and (token[1] + len(token)) <= end:\n",
    "                if curr_entity:\n",
    "                    tags.append('I-' + lbl)\n",
    "                else:\n",
    "                    tags.append('B-' + lbl)\n",
    "                    curr_entity = nxt_entity\n",
    "                if (token[1] + len(token)) >= end:\n",
    "                    curr_entity = None\n",
    "                    entities.pop(0)\n",
    "            else:\n",
    "                if token[1] >= end:\n",
    "                    entities.pop(0)\n",
    "                tags.append('O')\n",
    "                curr_entity = None\n",
    "        else:\n",
    "            tags.append('O')\n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_info = ['text', 'entities', 'tags', 'sentence_ids', 'tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = {}\n",
    "for info in file_info:\n",
    "    res[info] = []\n",
    "    for file_num in range(len(text_files)):\n",
    "        if info == 'text':\n",
    "            res[info].append(\"\")\n",
    "        else:\n",
    "            res[info].append([])\n",
    "        \n",
    "file_idx = {}\n",
    "for idx, file in enumerate(text_files):\n",
    "    file_idx[file] = idx\n",
    "        \n",
    "for entity in entities.itertuples():\n",
    "    file = entity[1]\n",
    "    idx = file_idx[file]\n",
    "    if res['text'][idx] == \"\":\n",
    "        res['text'][idx] = file_text[file]\n",
    "        res['sentence_ids'][idx], res['tokens'][idx] = tokenize(file)\n",
    "    # entity -> offset1, offset2, label, span\n",
    "    res['entities'][idx].append([entity[4], entity[5], entity[3], entity[6]])\n",
    "    \n",
    "for idx, file in enumerate(text_files):\n",
    "    text = file_text[file_name]\n",
    "    doc = nlp(text)\n",
    "    ents = res['entities'][idx].copy()\n",
    "    for i in res['sentence_ids'][idx]:\n",
    "        res['tags'][idx].append(get_bio_tags(res['tokens'][idx][i], ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_no = 1\n",
    "sentence_no = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 139),\n",
       " ('first', 143),\n",
       " ('of', 149),\n",
       " ('the', 152),\n",
       " ('corpses', 156),\n",
       " (',', 163),\n",
       " ('corresponding', 165),\n",
       " ('to', 179),\n",
       " ('the', 182),\n",
       " ('female', 186),\n",
       " (',', 192),\n",
       " ('was', 194),\n",
       " ('referred', 198),\n",
       " ('with', 207),\n",
       " ('the', 212),\n",
       " ('clinical', 216),\n",
       " ('judgement', 225),\n",
       " ('of', 235),\n",
       " ('severe', 238),\n",
       " ('respiratory', 245),\n",
       " ('failure', 257),\n",
       " ('with', 265),\n",
       " ('suspected', 270),\n",
       " ('Potter', 280),\n",
       " (\"'s\", 286),\n",
       " ('Syndrome', 289),\n",
       " ('and', 298),\n",
       " ('severe', 302),\n",
       " ('oligohydramnios', 309),\n",
       " (';', 324),\n",
       " ('she', 326),\n",
       " ('was', 330),\n",
       " ('born', 334),\n",
       " ('by', 339),\n",
       " ('emergency', 342),\n",
       " ('caesarean', 352),\n",
       " ('section', 362),\n",
       " ('due', 370),\n",
       " ('to', 374),\n",
       " ('breech', 377),\n",
       " ('presentation', 384),\n",
       " ('and', 397),\n",
       " ('the', 401),\n",
       " ('Apgar', 405),\n",
       " ('test', 411),\n",
       " ('was', 416),\n",
       " ('1/3/7', 420),\n",
       " (';', 425),\n",
       " ('minutes', 427),\n",
       " ('later', 435),\n",
       " ('she', 441),\n",
       " ('died', 445),\n",
       " ('.', 449)]"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['tokens'][file_no][sentence_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ENFERMEDAD',\n",
       " 'I-ENFERMEDAD',\n",
       " 'I-ENFERMEDAD',\n",
       " 'O',\n",
       " 'O',\n",
       " 'B-ENFERMEDAD',\n",
       " 'I-ENFERMEDAD',\n",
       " 'I-ENFERMEDAD',\n",
       " 'O',\n",
       " 'B-ENFERMEDAD',\n",
       " 'I-ENFERMEDAD',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O']"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['tags'][file_no][sentence_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['sentence_ids'][file_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[238, 264, 'ENFERMEDAD', 'severe respiratory failure'],\n",
       " [280, 297, 'ENFERMEDAD', \"Potter's Syndrome\"],\n",
       " [302, 324, 'ENFERMEDAD', 'severe oligohydramnios'],\n",
       " [556, 568, 'ENFERMEDAD', 'micrognathia'],\n",
       " [1057, 1062, 'ENFERMEDAD', 'cysts'],\n",
       " [1907, 1912, 'ENFERMEDAD', 'cysts'],\n",
       " [2004, 2009, 'ENFERMEDAD', 'cysts'],\n",
       " [2158, 2163, 'ENFERMEDAD', 'cysts'],\n",
       " [2262, 2267, 'ENFERMEDAD', 'cysts'],\n",
       " [2332, 2337, 'ENFERMEDAD', 'cysts'],\n",
       " [2252, 2267, 'ENFERMEDAD', 'medullary cysts'],\n",
       " [1004, 1021, 'ENFERMEDAD', 'cystic formations']]"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['entities'][file_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Two newborns, male and female from the same mother, died at 10 and 45 minutes of life respectively, and underwent post-mortem examination. The first of the corpses, corresponding to the female, was referred with the clinical judgement of severe respiratory failure with suspected Potter's Syndrome and severe oligohydramnios; she was born by emergency caesarean section due to breech presentation and the Apgar test was 1/3/7; minutes later she died. External examination revealed a subcyanotic colour, triangular facies with mongoloid parpebral fissures, micrognathia, broad nasal root and prominent occiput. The abdomen, globular, hard and slightly dented, allowed the palpation of two large masses occupying both renal fossae and hemiabdomenes. When the cavities were opened, the presence of two large renal masses measuring 10 x 8 x 5.5 cm and 12 x 8 x 6 cm with weights of 190 and 235 g respectively, stood out. Although the renal silhouette could be discerned, the surface, dented, showed numerous cystic formations of serous content; when cut, these cysts were heterogeneous in size, with those located at the cortical level being larger, giving the kidney a sponge-like appearance. The right and left lungs weighed 17 and 15 grams (usual weight of the set of 49 grams) showing a uniform reddish hue; both were compressed as a result of the diaphragmatic elevation conditioned by the large size of the kidneys. The rest of the organs showed no significant macroscopic alterations except for the positional alterations derived from the renal compression. In the second carcass, the male, similar morphological changes were observed, although the size of the kidneys was even larger, with weights of 300 and 310 grams. The rest of the abdominal viscera were compressed against the diaphragm. In both cases, a detailed histological study was performed, focusing especially on the kidneys where multiple cysts of different sizes with saccular morphology were demonstrated at the cortical level. These cysts occupied most of the corticomedullary parenchyma, although the preserved areas showed no significant alterations except for focal immaturity. These cysts were lined by a simple epithelium varying from flat to cubic. The smaller, more rounded medullary cysts were lined by a predominantly cubic epithelium. After the renal cysts, the most striking alterations were found in the liver where proliferation and dilatation, even cystic, of the bile ducts at the level of the portal spaces were observed. These findings led to a diagnosis of autosomal recessive polycystic kidney disease in both cases.\\n\\n\\n\""
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['text'][file_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "dump_file = 'data/processed_inp_data.json'\n",
    "\n",
    "with open(dump_file, 'w') as f:\n",
    "     f.write(json.dumps(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(dump_file) as json_file:\n",
    "#     data = json.load(json_file)\n",
    "\n",
    "# data['tags'][file_no][sentence_no]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
